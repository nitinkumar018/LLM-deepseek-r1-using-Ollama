# Mini Project: Run an LLM model locally on your laptop using Ollama (Local Deployment)
## Ollama is a platform that provides a local and private way to run LLM models on your PC, without internet, without relying on the cloud. So, it keeps the data on the local device and ensuring more privacy and security

# Steps:
# 1. visit ollama.com and download ollama 
# 2. go to 'Models' section and choose the LLM model you like. Suppose, we choose deepseek-r1 
# 3. select the no. of parameters for this model, let's say, we select 32b which means model has 32 billion parameters and at the same time, we can see the command for our desired model
# 4. copy the command and paste it on your terminal (Command Prompt)
# 5. run this command on your terminal which will install deepseek-r1 LLM model 
# 6. now, our LLM model is ready to chat 

# Ollama allows users to develop generative AI application using deepseek R1 model. For example, we can create private chatgpt using Ollama and deepseek R1 model which is open source and locally hosted. 
